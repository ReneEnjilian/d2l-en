{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n",
    "\n",
    ":label:`sec_finetuning-bert`\n",
    "\n",
    "\n",
    "In the previous sections of this chapter,\n",
    "we have designed different models for natural language processing applications,\n",
    "such as based on RNNs, CNNs, attention, and MLPs.\n",
    "These models are helpful when there is space or time constraint,\n",
    "however,\n",
    "crafting a specific model for every natural language processing task\n",
    "is practically infeasible.\n",
    "In :numref:`sec_bert`,\n",
    "we introduced a pretraining model, BERT,\n",
    "that requires minimal architecture changes\n",
    "for a wide range of natural language processing tasks.\n",
    "One one hand,\n",
    "at the time of its proposal,\n",
    "BERT improved the state of the art on various natural language processing tasks.\n",
    "On the other hand,\n",
    "as noted in :numref:`sec_bert-pretraining`,\n",
    "the two versions of the original BERT model\n",
    "come with 110 million and 340 million parameters.\n",
    "Thus, when there are sufficient computational resources,\n",
    "we may consider\n",
    "fine-tuning BERT for downstream natural language processing applications.\n",
    "\n",
    "In the following,\n",
    "we generalize a subset of natural language processing applications\n",
    "as sequence-level and token-level.\n",
    "On the sequence level,\n",
    "we introduce how to fine-tune BERT for single text classification and text pair classification, such as sentiment analysis and natural language inference,\n",
    "which we have already examined.\n",
    "On the token level, we will briefly introduce new applications\n",
    "such as text tagging and question answering\n",
    "and shed light on how BERT can fit in their problem settings.\n",
    "During fine-tuning,\n",
    "the \"minimal architecture changes\" required by BERT across different applications\n",
    "are the extra fully-connected layers.\n",
    "\n",
    "\n",
    "## Single Text Classification\n",
    "\n",
    "Single text classification takes a single text sequence as the input and returns its classification result as the output.\n",
    "Besides sentiment analysis,\n",
    "the Corpus of Linguistic Acceptability (CoLA)\n",
    "is a dataset for judging\n",
    "whether a given sentence is grammatically acceptable or not :cite:`Warstadt.Singh.Bowman.2019`.\n",
    "For instance, \"I should study.\" is acceptable but \"I should studying.\" is not.\n",
    "\n",
    "![Fine-tuning BERT for single text classification applications, such as sentiment analysis and testing linguistic acceptability.](../img/bert-one-seq.svg)\n",
    "\n",
    ":label:`fig_bert-one-seq`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Text Pair Classification\n",
    "\n",
    "![Fine-tuning BERT for text pair classification applications, such as natural language inference.](../img/bert-two-seqs.svg)\n",
    "\n",
    ":label:`fig_bert-two-seqs`\n",
    "\n",
    "\n",
    "\n",
    "## Text Tagging\n",
    "\n",
    "![Fine-tuning BERT for text tagging applications, such as part-of-speech tagging](../img/bert-tagging.svg)\n",
    "\n",
    ":label:`fig_bert-tagging`\n",
    "\n",
    "\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "![Fine-tuning BERT for question answering](../img/bert-qa.svg)\n",
    "\n",
    ":label:`fig_bert-qa`\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Fine-tune BERT.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Let us design a search engine algorithm for news articles. When the system receives an query (e.g., \"oil industry during the coronavirus outbreak\"), it should return a ranked list of news articles that are most relevant to the query. Suppose that we have a huge pool of news articles and a large number of queries. To simplify the problem, suppose that the most relevant article has been labeled for each query. How can we apply negative sampling (see :numref:`subsec_negative-sampling`) and BERT in the algorithm design?\n",
    "1. How can we leverage BERT in text generation tasks such as machine translation?\n",
    "\n",
    "\n",
    "\n",
    "## [Discussions](https://discuss.mxnet.io/t/5882)\n",
    "\n",
    "![](../img/qr_finetuning-bert.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}